{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BILL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HowardQian201/2022RiceDatathon/blob/main/BILL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "import csv\n",
        "import torchaudio\n",
        "import json\n",
        "import random\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "aW9mrPSwe7--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create graph dataset\n",
        "training_graph = pd.read_csv(\"/content/training_graph.csv\").values\n",
        "training_set = set(map(tuple, training_graph))\n",
        "\n",
        "isolated_nodes = pd.read_csv(\"/content/isolated_nodes.csv\").values\n",
        "isolated_nodes = set(isolated_nodes.flatten())\n",
        "\n",
        "node_set = random.sample(set(training_graph.flatten()).difference(isolated_nodes), 360)\n",
        "\n",
        "test_graph = torch.tensor(pd.read_csv(\"/content/test_edges.csv\").values)\n",
        "page_types = pd.read_csv(\"/content/node_classification.csv\", header=None, index_col=0,squeeze=True).to_dict()\n",
        "page_types.pop(\"id\")\n",
        "\n",
        "f = open(\"/content/node_features_text.json\")\n",
        "node_features = json.load(f)\n",
        "word_counts = Counter()\n",
        "for node in node_features.values():\n",
        "    #print(node)\n",
        "    # for word in node:\n",
        "    word_counts.update(node)\n",
        "delete = zip(*word_counts.most_common(50))\n",
        "# print(word_counts.most_common(50))\n",
        "\n",
        "for key in node_features.keys():\n",
        "    for num in node_features[key]:\n",
        "        if num in delete:\n",
        "            node_features[key].remove(num)\n",
        "    node_features[key] = set(node_features[key])\n",
        "\n",
        "# [ ((1,3),1), ((0,1),0),.... ]\n",
        "train_data = []\n",
        "for node1 in node_set:\n",
        "    for node2 in node_set:\n",
        "        if node1 != node2:\n",
        "            shared_feature_count = len(node_features[str(node1)].intersection(node_features[str(node2)]))\n",
        "            if (node1, node2) in training_set or (node2, node1) in training_set:\n",
        "                train_data.append((torch.tensor([1 if page_types[str(node1)] == page_types[str(node2)] else 0, shared_feature_count]).float(), numpy.float32(1)))\n",
        "            else:\n",
        "                train_data.append((torch.tensor([1 if page_types[str(node1)] == page_types[str(node2)] else 0, shared_feature_count]).float(), numpy.float32(0)))\n",
        "\n",
        "for edge in training_graph:\n",
        "    id1 = str(edge[0].item())\n",
        "    id2 = str(edge[1].item())\n",
        "    shared_feature_count = len(node_features[id1].intersection(node_features[id2]))\n",
        "    train_data.append((torch.tensor([1 if page_types[id1] == page_types[id2] else 0, shared_feature_count]).float(), numpy.float32(1)))\n",
        "\n",
        "# finished creating training data\n",
        "\n",
        "filtered = filter(lambda x: x[1] == 1, train_data)\n",
        "counter = 0\n",
        "for item in filtered:\n",
        "    #print(item)\n",
        "    counter += 1\n",
        "#print(counter)\n"
      ],
      "metadata": {
        "id": "c3j9dyoyhXRW"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = pd.read_csv(\"/content/test_labels.csv\").values\n",
        "test_edges = pd.read_csv(\"/content/test_edges.csv\").values\n",
        "\n",
        "test_data = []\n",
        "for i in range(len(test_edges)):\n",
        "    id1 = str(test_edges[i][0])\n",
        "    id2 = str(test_edges[i][1])\n",
        "    shared_feature_count = len(node_features[id1].intersection(node_features[id2]))\n",
        "    test_data.append((torch.tensor([1 if page_types[id1] == page_types[id2] else 0, shared_feature_count]).float(), numpy.float32(test_labels[i][0])))"
      ],
      "metadata": {
        "id": "Z7jIZBy1j4Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reserving 1000 as validation data\n",
        "print(len(train_data))\n",
        "train_ds, val_ds = random_split(train_data, [len(train_data) - 1000, 1000])\n",
        "print(type(train_data[0][1]))\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
        "val_loader = DataLoader(val_ds, batch_size*2, num_workers=2, pin_memory=True, drop_last=True)\n",
        "test_loader = DataLoader(test_data, batch_size*2, num_workers=2, pin_memory=True, drop_last=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwFFKdYpBKs_",
        "outputId": "82144327-4f1a-4f46-e25c-9594bcc852a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "261278\n",
            "<class 'numpy.float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TeacherModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # need to change initialization\n",
        "        # init_val = 0.9\n",
        "        self.linear = nn.Linear(2, 1)\n",
        "        # torch.nn.init.uniform_(self.linear.weight, -1 * init_val, init_val)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x.view(x.size(0), x.size(1))\n",
        "\n",
        "        out = self.linear(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = torch.flatten(self(images))  # Generate predictions\n",
        "        # print(\"here\", type(out[0].item()))\n",
        "        loss = F.mse_loss(out, labels)  # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = torch.flatten(self(images))                    # Generate predictions\n",
        "        loss = F.mse_loss(out, labels)   # Calculate loss\n",
        "        new_weights = torch.sqrt(self.linear.weight)\n",
        "        return {'val_loss': loss.detach()}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        return {'val_loss': epoch_loss.item()}\n",
        "\n",
        "    def calc_acc(self, predictions):\n",
        "        print('here')\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch, result['val_loss']))"
      ],
      "metadata": {
        "id": "QxRlCNz8hK3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_loader):\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.Adam):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        for batch in train_loader:\n",
        "            # print(type(batch[0][0][1].item()))\n",
        "            loss = model.training_step(batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ],
      "metadata": {
        "id": "vw7NshLRheOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "epochs = 10\n",
        "model = TeacherModel()\n",
        "history = [evaluate(model, val_loader)]\n",
        "print(history)\n",
        "history += fit(epochs, learning_rate, model, train_loader, val_loader)\n",
        "\n",
        "print(evaluate(model, test_loader))"
      ],
      "metadata": {
        "id": "umHro_8shihN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2678057-4d7b-4daf-b826-8c21bb22de5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'val_loss': 0.36310136318206787}]\n",
            "Epoch [0], val_loss: 0.1359\n",
            "Epoch [1], val_loss: 0.1367\n",
            "Epoch [2], val_loss: 0.1414\n",
            "Epoch [3], val_loss: 0.1345\n",
            "Epoch [4], val_loss: 0.1384\n",
            "Epoch [5], val_loss: 0.1482\n",
            "Epoch [6], val_loss: 0.1331\n",
            "Epoch [7], val_loss: 0.1490\n",
            "Epoch [8], val_loss: 0.1641\n",
            "Epoch [9], val_loss: 0.1451\n",
            "{'val_loss': 0.14541548490524292}\n"
          ]
        }
      ]
    }
  ]
}